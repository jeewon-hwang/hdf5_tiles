{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_zoom = 25\\ndsets = []\\nfor i in range(max_zoom):\\n    dsets += [f.create_dataset(\"zipped_{}\".format(i), (hum_size / 2 ** i,), dtype=\\'f\\', compression=\"gzip\")]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "filename = 'test.hdf5'\n",
    "\n",
    "if op.exists(filename):\n",
    "    os.remove(filename)\n",
    "    \n",
    "f = h5py.File(filename, 'w')\n",
    "hum_size = 3137161264\n",
    "#hum_size = 8000000\n",
    "\n",
    "'''\n",
    "max_zoom = 25\n",
    "dsets = []\n",
    "for i in range(max_zoom):\n",
    "    dsets += [f.create_dataset(\"zipped_{}\".format(i), (hum_size / 2 ** i,), dtype='f', compression=\"gzip\")]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding file values using a lookup table\n",
    "\n",
    "The majority of values in a BED-like file are not unique. If we can store them in a lookup table, then we can save space by using an `int16` rather than a `float32` to represent them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# take a subset of a file so we don't have to wait for the whole file to be loaded\n",
    "#zcat ~/data/encode/hg19/E002-H3K27me3.fc.signal.bigwig.genome.sorted.gz | head -n 1000000000 | gzip > /tmp/bgcompx1\n",
    "zcat E044-H3K27me3.fc.signal.bigwig.genome.sorted.gz | head -n 10000 | gzip > /tmp/bgcompx1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "import time as time\n",
    "\n",
    "# \n",
    "# where our subset resides\n",
    "filepath='/tmp/bgcompx1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to read: 0.025804758071899414\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import h5py\n",
    "\n",
    "t1 = time.time()\n",
    "hum_size = 3137161264\n",
    "\n",
    "\n",
    "with gzip.open(filepath, 'rt') as f:\n",
    "    # create an empty buffer the size of the human genome\n",
    "    reduced_buffer = np.zeros(hum_size, dtype='int16')\n",
    "    \n",
    "    lookup_table = {}\n",
    "    \n",
    "    for n,line in enumerate(csv.reader(f, delimiter='\\t')):\n",
    "        parts = line\n",
    "        \n",
    "        if parts[0] != 0:\n",
    "            # we ignore values with an entry of 0 because our array is already initialized to 0\n",
    "            num = float(parts[2])\n",
    "            \n",
    "            if num not in lookup_table:\n",
    "                lookup_table[num] = len(lookup_table)\n",
    "            \n",
    "            reduced_buffer[int(parts[0])-1:int(parts[1])-1] = lookup_table[num]\n",
    "                \n",
    "        '''\n",
    "        for i in range(int(parts[0]), int(parts[1])):\n",
    "            buffer1 += [float(parts[2]) - prev_value]\n",
    "            prev_value = float(parts[2])\n",
    "        '''\n",
    "print(\"time to read:\", time.time() - t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lookup_table) 1060\n"
     ]
    }
   ],
   "source": [
    "print(\"len(lookup_table)\", len(lookup_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 16bit data\n"
     ]
    }
   ],
   "source": [
    "filename = 'test1.hdf5'\n",
    "if op.exists(filename):\n",
    "    os.remove(filename)\n",
    "f = h5py.File(filename, 'w')\n",
    "hum_size = 3137161264\n",
    "\n",
    "if len(lookup_table) < 2 ** 16:\n",
    "    print(\"Creating 16bit data\")\n",
    "    dset = f.create_dataset('bedgraph', (hum_size,), dtype=np.int16, compression='gzip')\n",
    "else:\n",
    "    print(\"Creating 32bit data\")\n",
    "    dset = f.create_dataset('bedgraph', (hum_size,), dtype=np.int32, compression='gzip')\n",
    "\n",
    "dset[0:len(reduced_buffer)] = reduced_buffer\n",
    "\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_size: 8069484\n",
      "original file size: 62860\n"
     ]
    }
   ],
   "source": [
    "print(\"file_size:\", op.getsize(filename))\n",
    "print(\"original file size:\", op.getsize(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
